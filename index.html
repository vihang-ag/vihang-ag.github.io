<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  

  

  <title>Vihang Agarwal</title>
  
  <meta name="author" content="Vihang Agarwal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vihang Agarwal</name>
              </p>
              <p style="text-align:justify">I am a Research Assistant at University of Michigan, where I work on computer vision and deep learning for medical imaging advised by <a href="https://medicine.umich.edu/dept/radonc/research/research-laboratories/physics-laboratories/james-balter-laboratory">Prof. James Balter</a>. I recently completed my masters in EECS, Computer Vision from University of Michigan where I was fortunate enough to work with <a href="http://fouheylab.eecs.umich.edu/people.html">Prof. David Fouhey</a> on problems in Robot Navigation/3D visual scene understanding and compete in <a href="https://developer.amazon.com/alexaprize"> The Alexa Prize Socialbot Grand Challenge </a>. I also have an undergraduate degree in Mechanical Engineering from IIT Kanpur, India along with a minor in Artificial Intelligence.
              </p>
              <p style="text-align:center">
                <a href="mailto:vihang@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=uIl_34cAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/vihangagarwal/">LinkedIn</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests are in computer vision, deep learning, reinforcement learning, and medical imaging.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fastmri.jpg" alt="fastmri" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Accelerating MRI Acquisition using Cascaded Attention U-Net with Prior Information</papertitle>
              </a>
              <br>
              <em>AAPM/COMP</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://youtu.be/R3mtPHBVJG4">video</a>
              <p></p>
              <p>Reconstructing detail rich MRI using prior information from multi-imaging protocols and spatial non-local MR image patch regularities.</p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/alexa.png"><img src="images/alexa.png" alt="alexa" width="200" height="140" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexaprize/assets/challenge3/proceedings/U-Michigan-Audrey.pdf"> <papertitle>Audrey: A Personalized Open-Domain Conversational Bot</papertitle>
              </a>
              <br>
              <em>The Alexa Prize Proceedings</em>, 2020  
              <p></p>
              <p>An open-domain conversational chat-bot that aims to engage users on various conversational levels with socially-aware models such as Emotion Detection.</p>
            </td>
          </tr> 
    
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper.png"><img src="images/paper.png" alt="social_robot" width="200" height="140" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/ICICIC2017_Socialrobot_publication.pdf">
                <papertitle>A Narrative Speech, Gaze and Gesturing Robot Accessing to Human Emotion and Memory</papertitle>
              </a>
              <br>
              <em>International Conference on Innovative Computing, Information and Control</em>, 2017  
              <p></p>
              <p>A study of changes in attention and episodic memory of human participants due to Human-Robot social interactions.</p>
            </td>
          </tr>  
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/pawpal.png"><img src="images/pawpal.png" alt="pawpal" width="200" height="140" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>PawPal</strong>
              <br>
              <em>EECS 504: Foundations of Computer Vision</em>
              <br>
              <em>Instructor: <a href="https://web.eecs.umich.edu/~jjcorso/"> Dr. Jason Corso</a> </em>
              <br>
              <a href="data/504_poster.pdf">poster</a> /
              <a href="https://github.com/ehofesmann/PawPal">code</a> /
              <a href="data/ProjectReport_ComputerVision.pdf">report</a>
              <p></p>
              <p>Using state-of-the-art computer vision algorithms, we developed a dog localization and activity recognition system that can determine what your dog is doing from a home surveillance camera. An innovative solution allowing autonomous active monitoring and safekeeping of pets without requiring any interference from the owner.</p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/trafficflow.png"><img src="images/trafficflow.png" alt="trafficflow" width="200" height="140" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Improving Traffic Flow with Deep RL</strong>
              <br>
              <em>EECS 545: Machine Learning</em>
              <br>
              <em>Instructor: <a href="https://web.eecs.umich.edu/~cscott/"> Clayton Scott</a> </em>
              <br>
              <a href="https://github.com/nmtvijay/Improving-Traffic-Flow-with-Deep-RL">code</a> /
              <a href="data/ProjectReport_MachineLearning.pdf">report</a>
              <p></p>
              <p>We compared Action-specific Deep Q networks (ADQN) and Action specific Recurrent Deep Q networks (ADRQN) to drive vehicles in the Deep Traffic Simulator. The recurrent network can handle partial observability induced by only having access to a partial field of view per timestep and converges faster in this setting.</p>
            </td>
          </tr> 

          <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='uflow_image'>
                  <img src='images/visualscene_depth.png' width="200" height="140"></div>
                <img src='images/visualscene_before.png' width="200" height="140">
              </div>
              <script type="text/javascript">
                function uflow_start() {
                  document.getElementById('uflow_image').style.opacity = "1";
                }

                function uflow_stop() {
                  document.getElementById('uflow_image').style.opacity = "0";
                }
                uflow_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>3D Visual Scene Understanding</strong>
              <br>
              <em>Independent Study</em>
              <br>
              <em>Instructor: <a href="http://fouheylab.eecs.umich.edu/people.html"> David Fouhey</a> </em>
               
              <p></p>
              <p>I implemented a ResNet-DenseNet encoder-decoder network for estimating depth maps, normals and occlusion edges from single images on the NYUv2 dataset. The hope was to exploit learnt feature representations and intra task dependencies for efficient transfer learning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/greedy.png" alt="greedy" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Gluttonous: A greedy Algorithm for Steiner Forest</strong>
              <br>
              <em>EECS 598: Approximation Algorithms</em>
              <br>
              <em>Instructor: <a href="https://viswa.engin.umich.edu/"> Viswanath Nagarajan</a> </em>
              <br>
              <a href="data/GreedySteinerForest_presentation.pdf">presentation</a> /
              <a href="data/Greedy_Algorithms_for_Steiner_Forest_report.pdf">report</a> /
              <a href="https://arxiv.org/pdf/1412.7693.pdf">original arxiv paper</a>
              <p></p>
              <p>We summarized the core ideas behind Greedy Algorithms for solving Steiner Forest problems in a short technical report and presented the intuition behind these algorithms in an easily understandable and comprehensive presentation.</p>
            </td>
          </tr>
    
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/humanai.png"><img src="images/humanai.png" alt="humanai" width="200" height="140" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Lazy Human AI Teams</strong>
              <br>
              <em>EECS 598: Human-AI Interaction and Crowdsourcing</em>
              <br>
              <em>Instructor: <a href="https://robotics.umich.edu/profile/walter-s-lasecki/"> Walter S. Lasecki</a> </em>
              <br>
              <a href="https://github.com/vihang-ag/LazyAIteams">code</a> /
              <a href="data/ProjectReport_InteractiveAI.pdf">report</a>
              <p></p>
              <p>We explored the use of Human Effort as an additional task dimension in Human-AI teams and showed that optimising Human Effort in an active learning paradigm improves overall team performance. I simulated a variety of active learning experiments on the task of text annotation and designed a simple web interface to conduct experiments.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/policysketch.png" alt="policysketch" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>Generalizing Navigation Behaviors with Policy Sketches</strong>
              <br>
              <em>Independent Study</em>
              <br>
              <em>Instructor: <a href="http://fouheylab.eecs.umich.edu/people.html"> David Fouhey</a> </em>
              <br>
              <a href="data/proposal_project.pdf">proposed abstract</a> 
              <p></p>
              <p>We explored long range 3D Navigation in the virtual House 3D environment. I defined behavioral primitives important for navigation at large scales and implemented Imitation Learning to learn these primitives as <a href="https://arxiv.org/abs/1611.01796"> policy sketches</a> through supervised demonstrations. The goal was to learn a general navigation policy as a sequence of such policy sketches.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/painting.png" alt="painting" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>VAE for Painting Timeline</strong>
              <br>
              <em>CS 771A: Introduction to Machine Learning</em>
              <br>
              <em>Instructor: <a href="https://www.cse.iitk.ac.in/users/purushot/"> Purushottam Kar</a> </em>
              <br>
              <a href="data/ML_term_paper.pdf">report</a> 
              <p></p>
              <p>The current architecture of VAE suffers from latent space saturation (with inefficient packing) and mode collapse. We aimed to deal with mode collapse by introducing more than one encoder in the architecture, with the latent spaces mapped to a single decoder. We attempted to incorporate different techniques to get good reconstructions.</p>
            </td>
          </tr>  
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
            	<img src="images/ia.jpg" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>SI 301: Models of Social Information Processing - Instructional Aide</p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Thanks Jon for this amazing template.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

<!-- The idea of machines that can collaborate with us, augmenting our capabilities and enabling us to attain superhuman performances fascinates me. Consequently, I want to develop algorithms that use ideas from computer vision and Human-AI interaction to build intelligent agents.
 -->
